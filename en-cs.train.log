Loading train dataset from /home/ubuntu/lmm-data/en-cs/demo.train.1.pt, number of examples: 92507
Source data type: words
Target data type: characters
 * vocabulary size. source = 15576
Building model...
Initializing model parameters.
NMTTargetCharModel(
  (encoder): RNNEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(15576, 512, padding_idx=1)
        )
        (dropout): Dropout(p=0.2, inplace=False)
      )
    )
    (rnn): GRU(512, 256, dropout=0.2, bidirectional=True)
  )
  (decoder1): StdWordRNNDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(1004, 512, padding_idx=1)
        )
        (dropout): Dropout(p=0.2, inplace=False)
      )
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (rnn): StackedGRU(
      (dropout): Dropout(p=0.2, inplace=False)
      (layers): ModuleList(
        (0): GRUCell(1024, 512)
      )
    )
    (attn): GlobalAttention(
      (linear_in): Linear(in_features=512, out_features=512, bias=False)
      (linear_out): Linear(in_features=1024, out_features=512, bias=False)
      (sm): Softmax(dim=1)
      (tanh): Tanh()
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (char_rnn): GRU(512, 256, dropout=0.2, bidirectional=True)
    (combine_states): Linear(in_features=512, out_features=512, bias=True)
    (word2char): Linear(in_features=1536, out_features=512, bias=True)
    (tanh): Tanh()
    (sampler_z): DiagonalGaussianSampler(
      (mu): MLP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (tanh): Tanh()
        (fc2): Linear(in_features=256, out_features=100, bias=True)
      )
      (sigma): MLP_SP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (tanh): Tanh()
        (fc2): Linear(in_features=256, out_features=100, bias=True)
        (softplus): Softplus(beta=1, threshold=20)
      )
    )
    (substract): Linear(in_features=100, out_features=512, bias=True)
    (sampler_f): KumaSampler(
      (na): MLP_SP(
        (fc1): Linear(in_features=612, out_features=306, bias=True)
        (tanh): Tanh()
        (fc2): Linear(in_features=306, out_features=10, bias=True)
        (softplus): Softplus(beta=1, threshold=20)
      )
      (nb): MLP_SP(
        (fc1): Linear(in_features=612, out_features=306, bias=True)
        (tanh): Tanh()
        (fc2): Linear(in_features=306, out_features=10, bias=True)
        (softplus): Softplus(beta=1, threshold=20)
      )
    )
    (wordcomposition): Linear(in_features=110, out_features=512, bias=True)
  )
  (decoder2): StdCharRNNDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(1004, 512, padding_idx=1)
        )
        (dropout): Dropout(p=0.2, inplace=False)
      )
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (char_decoder): StackedGRU(
      (dropout): Dropout(p=0.2, inplace=False)
      (layers): ModuleList(
        (0): GRUCell(512, 512)
      )
    )
  )
  (generator): Sequential(
    (0): Linear(in_features=512, out_features=1004, bias=True)
    (1): LogSoftmax()
  )
)
* number of parameters: 17947684
encoder:  9157632
decoder:  8790052
Making optimizer for training.

Start training...
 * number of epochs: 1, starting from Epoch 1
 * batch size: 64

Loading train dataset from /home/ubuntu/lmm-data/en-cs/demo.train.1.pt, number of examples: 92507
Epoch  1,    50/ 1446; acc:  59.16; ppl:  17.39; 1249 src tok/s; 2113 tgt tok/s;     54 s elapsed
Epoch  1,   100/ 1446; acc:  66.18; ppl:   7.54; 1377 src tok/s; 2327 tgt tok/s;    100 s elapsed
Epoch  1,   150/ 1446; acc:  67.89; ppl:   6.86; 1316 src tok/s; 2239 tgt tok/s;    149 s elapsed
Epoch  1,   200/ 1446; acc:  67.99; ppl:   6.36; 1379 src tok/s; 2337 tgt tok/s;    197 s elapsed
Epoch  1,   250/ 1446; acc:  68.34; ppl:   6.15; 1292 src tok/s; 2186 tgt tok/s;    249 s elapsed
Epoch  1,   300/ 1446; acc:  69.47; ppl:   5.78; 1372 src tok/s; 2330 tgt tok/s;    295 s elapsed
Epoch  1,   350/ 1446; acc:  70.42; ppl:   5.48; 1327 src tok/s; 2267 tgt tok/s;    338 s elapsed
Epoch  1,   400/ 1446; acc:  70.21; ppl:   5.54; 1393 src tok/s; 2334 tgt tok/s;    390 s elapsed
Epoch  1,   450/ 1446; acc:  70.96; ppl:   5.24; 1377 src tok/s; 2332 tgt tok/s;    438 s elapsed
Epoch  1,   500/ 1446; acc:  71.11; ppl:   5.20; 1311 src tok/s; 2235 tgt tok/s;    487 s elapsed
Epoch  1,   550/ 1446; acc:  71.70; ppl:   4.99; 1336 src tok/s; 2320 tgt tok/s;    527 s elapsed
Epoch  1,   600/ 1446; acc:  70.76; ppl:   5.19; 1328 src tok/s; 2218 tgt tok/s;    585 s elapsed
Epoch  1,   650/ 1446; acc:  71.57; ppl:   4.97; 1328 src tok/s; 2251 tgt tok/s;    634 s elapsed
Epoch  1,   700/ 1446; acc:  71.98; ppl:   4.79; 1409 src tok/s; 2384 tgt tok/s;    680 s elapsed
Epoch  1,   750/ 1446; acc:  71.65; ppl:   4.85; 1399 src tok/s; 2343 tgt tok/s;    731 s elapsed
Epoch  1,   800/ 1446; acc:  72.58; ppl:   4.61; 1307 src tok/s; 2239 tgt tok/s;    775 s elapsed
Epoch  1,   850/ 1446; acc:  72.32; ppl:   4.65; 1334 src tok/s; 2251 tgt tok/s;    826 s elapsed
Epoch  1,   900/ 1446; acc:  72.78; ppl:   4.47; 1356 src tok/s; 2303 tgt tok/s;    872 s elapsed
Epoch  1,   950/ 1446; acc:  72.43; ppl:   4.57; 1329 src tok/s; 2251 tgt tok/s;    923 s elapsed
Epoch  1,  1000/ 1446; acc:  72.94; ppl:   4.40; 1355 src tok/s; 2298 tgt tok/s;    969 s elapsed
Epoch  1,  1050/ 1446; acc:  73.10; ppl:   4.35; 1339 src tok/s; 2293 tgt tok/s;   1015 s elapsed
Epoch  1,  1100/ 1446; acc:  73.01; ppl:   4.35; 1350 src tok/s; 2272 tgt tok/s;   1065 s elapsed
Epoch  1,  1150/ 1446; acc:  72.69; ppl:   4.40; 1363 src tok/s; 2305 tgt tok/s;   1116 s elapsed
Epoch  1,  1200/ 1446; acc:  73.65; ppl:   4.16; 1382 src tok/s; 2362 tgt tok/s;   1160 s elapsed
Epoch  1,  1250/ 1446; acc:  73.54; ppl:   4.14; 1404 src tok/s; 2373 tgt tok/s;   1207 s elapsed
Epoch  1,  1300/ 1446; acc:  73.41; ppl:   4.15; 1397 src tok/s; 2373 tgt tok/s;   1253 s elapsed
Epoch  1,  1350/ 1446; acc:  73.78; ppl:   4.06; 1392 src tok/s; 2355 tgt tok/s;   1301 s elapsed
Epoch  1,  1400/ 1446; acc:  73.53; ppl:   4.06; 1307 src tok/s; 2238 tgt tok/s;   1350 s elapsed
Train perplexity: 5.13524
Train accuracy: 71.1094
Loading valid dataset from /home/ubuntu/lmm-data/en-cs/demo.valid.1.pt, number of examples: 3004
Validation perplexity: 4.14456
Validation accuracy: 72.8691
Decaying learning rate to 0.00027
