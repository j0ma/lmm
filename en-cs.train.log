Loading train dataset from /home/ubuntu/lmm-data/en-cs/demo.train.1.pt, number of examples: 92507
Source data type: words
Target data type: characters
 * vocabulary size. source = 15576
Building model...
Initializing model parameters.
NMTTargetCharModel(
  (encoder): RNNEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(15576, 512, padding_idx=1)
        )
        (dropout): Dropout(p=0.2, inplace=False)
      )
    )
    (rnn): GRU(512, 256, dropout=0.2, bidirectional=True)
  )
  (decoder1): StdWordRNNDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(1004, 512, padding_idx=1)
        )
        (dropout): Dropout(p=0.2, inplace=False)
      )
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (rnn): StackedGRU(
      (dropout): Dropout(p=0.2, inplace=False)
      (layers): ModuleList(
        (0): GRUCell(1024, 512)
      )
    )
    (attn): GlobalAttention(
      (linear_in): Linear(in_features=512, out_features=512, bias=False)
      (linear_out): Linear(in_features=1024, out_features=512, bias=False)
      (sm): Softmax(dim=1)
      (tanh): Tanh()
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (char_rnn): GRU(512, 256, dropout=0.2, bidirectional=True)
    (combine_states): Linear(in_features=512, out_features=512, bias=True)
    (word2char): Linear(in_features=1536, out_features=512, bias=True)
    (tanh): Tanh()
    (sampler_z): DiagonalGaussianSampler(
      (mu): MLP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (tanh): Tanh()
        (fc2): Linear(in_features=256, out_features=100, bias=True)
      )
      (sigma): MLP_SP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (tanh): Tanh()
        (fc2): Linear(in_features=256, out_features=100, bias=True)
        (softplus): Softplus(beta=1, threshold=20)
      )
    )
    (substract): Linear(in_features=100, out_features=512, bias=True)
    (sampler_f): KumaSampler(
      (na): MLP_SP(
        (fc1): Linear(in_features=612, out_features=306, bias=True)
        (tanh): Tanh()
        (fc2): Linear(in_features=306, out_features=10, bias=True)
        (softplus): Softplus(beta=1, threshold=20)
      )
      (nb): MLP_SP(
        (fc1): Linear(in_features=612, out_features=306, bias=True)
        (tanh): Tanh()
        (fc2): Linear(in_features=306, out_features=10, bias=True)
        (softplus): Softplus(beta=1, threshold=20)
      )
    )
    (wordcomposition): Linear(in_features=110, out_features=512, bias=True)
  )
  (decoder2): StdCharRNNDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(1004, 512, padding_idx=1)
        )
        (dropout): Dropout(p=0.2, inplace=False)
      )
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (char_decoder): StackedGRU(
      (dropout): Dropout(p=0.2, inplace=False)
      (layers): ModuleList(
        (0): GRUCell(512, 512)
      )
    )
  )
  (generator): Sequential(
    (0): Linear(in_features=512, out_features=1004, bias=True)
    (1): LogSoftmax()
  )
)
* number of parameters: 17947684
encoder:  9157632
decoder:  8790052
Making optimizer for training.

Start training...
 * number of epochs: 1, starting from Epoch 1
 * batch size: 64

Loading train dataset from /home/ubuntu/lmm-data/en-cs/demo.train.1.pt, number of examples: 92507
Epoch  1,    50/ 1446; acc:  59.13; ppl:  17.90; 1211 src tok/s; 2049 tgt tok/s;     56 s elapsed
Epoch  1,   100/ 1446; acc:  66.71; ppl:   7.53; 1356 src tok/s; 2310 tgt tok/s;    103 s elapsed
Epoch  1,   150/ 1446; acc:  67.90; ppl:   6.77; 1260 src tok/s; 2132 tgt tok/s;    153 s elapsed
Epoch  1,   200/ 1446; acc:  68.39; ppl:   6.36; 1380 src tok/s; 2350 tgt tok/s;    201 s elapsed
Epoch  1,   250/ 1446; acc:  68.52; ppl:   6.03; 1264 src tok/s; 2134 tgt tok/s;    254 s elapsed
Epoch  1,   300/ 1446; acc:  69.58; ppl:   5.78; 1356 src tok/s; 2302 tgt tok/s;    300 s elapsed
Epoch  1,   350/ 1446; acc:  70.37; ppl:   5.48; 1351 src tok/s; 2312 tgt tok/s;    343 s elapsed
Epoch  1,   400/ 1446; acc:  69.95; ppl:   5.57; 1375 src tok/s; 2309 tgt tok/s;    396 s elapsed
Epoch  1,   450/ 1446; acc:  70.84; ppl:   5.30; 1360 src tok/s; 2310 tgt tok/s;    445 s elapsed
Epoch  1,   500/ 1446; acc:  71.05; ppl:   5.19; 1279 src tok/s; 2187 tgt tok/s;    494 s elapsed
Epoch  1,   550/ 1446; acc:  71.84; ppl:   4.92; 1320 src tok/s; 2263 tgt tok/s;    535 s elapsed
Epoch  1,   600/ 1446; acc:  71.05; ppl:   5.10; 1380 src tok/s; 2318 tgt tok/s;    591 s elapsed
Epoch  1,   650/ 1446; acc:  71.61; ppl:   4.96; 1332 src tok/s; 2261 tgt tok/s;    640 s elapsed
Epoch  1,   700/ 1446; acc:  72.06; ppl:   4.80; 1360 src tok/s; 2305 tgt tok/s;    688 s elapsed
Epoch  1,   750/ 1446; acc:  71.78; ppl:   4.83; 1385 src tok/s; 2322 tgt tok/s;    740 s elapsed
Epoch  1,   800/ 1446; acc:  72.41; ppl:   4.67; 1278 src tok/s; 2179 tgt tok/s;    786 s elapsed
Epoch  1,   850/ 1446; acc:  72.29; ppl:   4.65; 1316 src tok/s; 2228 tgt tok/s;    837 s elapsed
Epoch  1,   900/ 1446; acc:  72.41; ppl:   4.55; 1328 src tok/s; 2254 tgt tok/s;    884 s elapsed
Epoch  1,   950/ 1446; acc:  72.53; ppl:   4.51; 1360 src tok/s; 2288 tgt tok/s;    934 s elapsed
Epoch  1,  1000/ 1446; acc:  72.70; ppl:   4.45; 1354 src tok/s; 2304 tgt tok/s;    980 s elapsed
Epoch  1,  1050/ 1446; acc:  72.98; ppl:   4.34; 1348 src tok/s; 2291 tgt tok/s;   1026 s elapsed
Epoch  1,  1100/ 1446; acc:  72.75; ppl:   4.40; 1344 src tok/s; 2272 tgt tok/s;   1076 s elapsed
Epoch  1,  1150/ 1446; acc:  72.54; ppl:   4.44; 1310 src tok/s; 2199 tgt tok/s;   1130 s elapsed
Epoch  1,  1200/ 1446; acc:  73.68; ppl:   4.16; 1338 src tok/s; 2277 tgt tok/s;   1175 s elapsed
Epoch  1,  1250/ 1446; acc:  73.40; ppl:   4.22; 1373 src tok/s; 2319 tgt tok/s;   1223 s elapsed
Epoch  1,  1300/ 1446; acc:  73.55; ppl:   4.14; 1286 src tok/s; 2176 tgt tok/s;   1274 s elapsed
Epoch  1,  1350/ 1446; acc:  73.56; ppl:   4.09; 1356 src tok/s; 2301 tgt tok/s;   1323 s elapsed
Epoch  1,  1400/ 1446; acc:  73.75; ppl:   4.04; 1299 src tok/s; 2223 tgt tok/s;   1371 s elapsed
Train perplexity: 5.15015
Train accuracy: 71.1011
Loading valid dataset from /home/ubuntu/lmm-data/en-cs/demo.valid.1.pt, number of examples: 3004
Validation perplexity: 4.20621
Validation accuracy: 72.6968
Decaying learning rate to 0.00027
