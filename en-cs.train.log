Loading train dataset from /home/ubuntu/lmm-data/en-cs/demo.train.1.pt, number of examples: 92507
Source data type: words
Target data type: characters
 * vocabulary size. source = 15576
Building model...
About to make encoder
opt.rnn_type=GRU
opt.brnn=True
opt.enc_layers=1
opt.rnn_size =512
opt.dropout=0.2
embeddings=Embeddings(
  (make_embedding): Sequential(
    (emb_luts): Elementwise(
      (0): Embedding(15576, 512, padding_idx=1)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
opt.bridge=False
This is make_decoder()
opt.rnn_type=GRU
opt.brnn=True
opt.dec_lay=3
opt.rnn_size=512
opt.global_attention=general
opt.coverage_attn=False
opt.context_gate=None
opt.copy_attn=False
opt.dropout=0.2
embeddings=Embeddings(
  (make_embedding): Sequential(
    (emb_luts): Elementwise(
      (0): Embedding(1004, 512, padding_idx=1)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
opt.reuse_copy_attn=False
Initializing model parameters.
NMTTargetCharModel(
  (encoder): RNNEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(15576, 512, padding_idx=1)
        )
        (dropout): Dropout(p=0.2, inplace=False)
      )
    )
    (rnn): GRU(512, 256, dropout=0.2, bidirectional=True)
  )
  (decoder1): StdWordRNNDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(1004, 512, padding_idx=1)
        )
        (dropout): Dropout(p=0.2, inplace=False)
      )
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (rnn): StackedGRU(
      (dropout): Dropout(p=0.2, inplace=False)
      (layers): ModuleList(
        (0): GRUCell(1024, 512)
        (1): GRUCell(512, 512)
        (2): GRUCell(512, 512)
      )
    )
    (attn): GlobalAttention(
      (linear_in): Linear(in_features=512, out_features=512, bias=False)
      (linear_out): Linear(in_features=1024, out_features=512, bias=False)
      (sm): Softmax(dim=1)
      (tanh): Tanh()
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (char_rnn): GRU(512, 256, dropout=0.2, bidirectional=True)
    (combine_states): Linear(in_features=512, out_features=512, bias=True)
    (word2char): Linear(in_features=1536, out_features=512, bias=True)
    (tanh): Tanh()
    (sampler_z): DiagonalGaussianSampler(
      (mu): MLP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (tanh): Tanh()
        (fc2): Linear(in_features=256, out_features=100, bias=True)
      )
      (sigma): MLP_SP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (tanh): Tanh()
        (fc2): Linear(in_features=256, out_features=100, bias=True)
        (softplus): Softplus(beta=1, threshold=20)
      )
    )
    (substract): Linear(in_features=100, out_features=512, bias=True)
    (sampler_f): KumaSampler(
      (na): MLP_SP(
        (fc1): Linear(in_features=612, out_features=306, bias=True)
        (tanh): Tanh()
        (fc2): Linear(in_features=306, out_features=10, bias=True)
        (softplus): Softplus(beta=1, threshold=20)
      )
      (nb): MLP_SP(
        (fc1): Linear(in_features=612, out_features=306, bias=True)
        (tanh): Tanh()
        (fc2): Linear(in_features=306, out_features=10, bias=True)
        (softplus): Softplus(beta=1, threshold=20)
      )
    )
    (wordcomposition): Linear(in_features=110, out_features=512, bias=True)
  )
  (decoder2): StdCharRNNDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(1004, 512, padding_idx=1)
        )
        (dropout): Dropout(p=0.2, inplace=False)
      )
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (char_decoder): StackedGRU(
      (dropout): Dropout(p=0.2, inplace=False)
      (layers): ModuleList(
        (0): GRUCell(512, 512)
      )
    )
  )
  (generator): Sequential(
    (0): Linear(in_features=512, out_features=1004, bias=True)
    (1): LogSoftmax()
  )
)
* number of parameters: 21099556
encoder:  9157632
decoder:  11941924
Making optimizer for training.

Start training...
 * number of epochs: 1, starting from Epoch 1
 * batch size: 64

Loading train dataset from /home/ubuntu/lmm-data/en-cs/demo.train.1.pt, number of examples: 92507
