WARNING: You have a CUDA device, should run with -gpuid 0
Loading train dataset from /home/think/lmm-data/en-tr/demo.train.1.pt, number of examples: 157575
Source data type: words
Target data type: characters
 * vocabulary size. source = 15770
Building model...
Initializing model parameters.
NMTTargetCharModel(
  (encoder): RNNEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(15770, 512, padding_idx=1)
        )
        (dropout): Dropout(p=0.2)
      )
    )
    (rnn): GRU(512, 256, dropout=0.2, bidirectional=True)
  )
  (decoder1): StdWordRNNDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(1004, 512, padding_idx=1)
        )
        (dropout): Dropout(p=0.2)
      )
    )
    (dropout): Dropout(p=0.2)
    (rnn): StackedGRU(
      (dropout): Dropout(p=0.2)
      (layers): ModuleList(
        (0): GRUCell(1024, 512)
      )
    )
    (attn): GlobalAttention(
      (linear_in): Linear(in_features=512, out_features=512, bias=False)
      (linear_out): Linear(in_features=1024, out_features=512, bias=False)
      (sm): Softmax()
      (tanh): Tanh()
      (dropout): Dropout(p=0.0)
    )
    (char_rnn): GRU(512, 256, dropout=0.2, bidirectional=True)
    (combine_states): Linear(in_features=512, out_features=512, bias=True)
    (word2char): Linear(in_features=1536, out_features=512, bias=True)
    (tanh): Tanh()
    (sampler_z): DiagonalGaussianSampler(
      (mu): MLP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (tanh): Tanh()
        (fc2): Linear(in_features=256, out_features=100, bias=True)
      )
      (sigma): MLP_SP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (tanh): Tanh()
        (fc2): Linear(in_features=256, out_features=100, bias=True)
        (softplus): Softplus(beta=1, threshold=20)
      )
    )
    (substract): Linear(in_features=100, out_features=512, bias=True)
    (sampler_f): KumaSampler(
      (na): MLP_SP(
        (fc1): Linear(in_features=612, out_features=306, bias=True)
        (tanh): Tanh()
        (fc2): Linear(in_features=306, out_features=10, bias=True)
        (softplus): Softplus(beta=1, threshold=20)
      )
      (nb): MLP_SP(
        (fc1): Linear(in_features=612, out_features=306, bias=True)
        (tanh): Tanh()
        (fc2): Linear(in_features=306, out_features=10, bias=True)
        (softplus): Softplus(beta=1, threshold=20)
      )
    )
    (wordcomposition): Linear(in_features=110, out_features=512, bias=True)
  )
  (decoder2): StdCharRNNDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(1004, 512, padding_idx=1)
        )
        (dropout): Dropout(p=0.2)
      )
    )
    (dropout): Dropout(p=0.2)
    (char_decoder): StackedGRU(
      (dropout): Dropout(p=0.2)
      (layers): ModuleList(
        (0): GRUCell(512, 512)
      )
    )
  )
  (generator): Sequential(
    (0): Linear(in_features=512, out_features=1004, bias=True)
    (1): LogSoftmax()
  )
)
* number of parameters: 18047012
encoder:  9256960
decoder:  8790052
Making optimizer for training.

Start training...
 * number of epochs: 200, starting from Epoch 1
 * batch size: 64

Loading train dataset from /home/think/lmm-data/en-tr/demo.train.1.pt, number of examples: 157575
