Building model...

# ENCODER
opt.rnn_type=GRU
opt.brnn=True
opt.enc_layers=1
opt.rnn_size =512
opt.dropout=0.2
embeddings=Embeddings(
  (make_embedding): Sequential(
    (emb_luts): Elementwise(
      (0): Embedding(15576, 512, padding_idx=1)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
opt.bridge=False

# DECODER 
This is make_decoder()

"""
So given all this, it seems like;
- There are 2 decoders with the same options
    - StdWordRNNDecoder & StdCharRNNDecoder
- There is only 1 decoding layer
    - Wat, this doesn't go according to the experiments?
"""

opt.rnn_type=GRU                                # What kind of RNN?
opt.brnn=True                                   # Should it be bidirectional?
opt.dec_lay=1                                   # How many dconding layers?
opt.rnn_size=512                                # Size of hidden dimension?
opt.global_attention=general                    # What kind of attention?
opt.coverage_attn=False                         # ------------------------
opt.context_gate=None                           # ------------------------
opt.copy_attn=False                             # ------------------------
opt.dropout=0.2                                 # What is the dropout prob?
embeddings=Embeddings(                          # 512 dimensional embeddings 
  (make_embedding): Sequential(
    (emb_luts): Elementwise(                    # emb_luts = lookup tables
      (0): Embedding(1004, 512, padding_idx=1)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
opt.reuse_copy_attn=False                       # ------------------------

# ACTUAL MODEL 
NMTTargetCharModel(
  (encoder): RNNEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(15576, 512, padding_idx=1)
        )
        (dropout): Dropout(p=0.2, inplace=False)
      )
    )
    (rnn): GRU(512, 256, dropout=0.2, bidirectional=True)
  )
  (decoder1): StdWordRNNDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(1004, 512, padding_idx=1)
        )
        (dropout): Dropout(p=0.2, inplace=False)
      )
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (rnn): StackedGRU(
      (dropout): Dropout(p=0.2, inplace=False)
      (layers): ModuleList(
        (0): GRUCell(1024, 512)
      )
    )
    (attn): GlobalAttention(
      (linear_in): Linear(in_features=512, out_features=512, bias=False)
      (linear_out): Linear(in_features=1024, out_features=512, bias=False)
      (sm): Softmax(dim=1)
      (tanh): Tanh()
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (char_rnn): GRU(512, 256, dropout=0.2, bidirectional=True)
    (combine_states): Linear(in_features=512, out_features=512, bias=True)
    (word2char): Linear(in_features=1536, out_features=512, bias=True)
    (tanh): Tanh()
    (sampler_z): DiagonalGaussianSampler(
      (mu): MLP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (tanh): Tanh()
        (fc2): Linear(in_features=256, out_features=100, bias=True)
      )
      (sigma): MLP_SP(
        (fc1): Linear(in_features=512, out_features=256, bias=True)
        (tanh): Tanh()
        (fc2): Linear(in_features=256, out_features=100, bias=True)
        (softplus): Softplus(beta=1, threshold=20)
      )
    )
    (substract): Linear(in_features=100, out_features=512, bias=True)
    (sampler_f): KumaSampler(
      (na): MLP_SP(
        (fc1): Linear(in_features=612, out_features=306, bias=True)
        (tanh): Tanh()
        (fc2): Linear(in_features=306, out_features=10, bias=True)
        (softplus): Softplus(beta=1, threshold=20)
      )
      (nb): MLP_SP(
        (fc1): Linear(in_features=612, out_features=306, bias=True)
        (tanh): Tanh()
        (fc2): Linear(in_features=306, out_features=10, bias=True)
        (softplus): Softplus(beta=1, threshold=20)
      )
    )
    (wordcomposition): Linear(in_features=110, out_features=512, bias=True)
  )
  (decoder2): StdCharRNNDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(1004, 512, padding_idx=1)
        )
        (dropout): Dropout(p=0.2, inplace=False)
      )
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (char_decoder): StackedGRU(
      (dropout): Dropout(p=0.2, inplace=False)
      (layers): ModuleList(
        (0): GRUCell(512, 512)
      )
    )
  )
  (generator): Sequential(
    (0): Linear(in_features=512, out_features=1004, bias=True)
    (1): LogSoftmax()
  )
)
